\documentclass{article}

\usepackage[final,nonatbib]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage[backend=biber]{biblatex}
\addbibresource{report.bib}

\usepackage{graphicx}
\graphicspath{ {images/} }

% https://tex.stackexchange.com/questions/376420/include-chinese-characters-into-article-in-xelatex
\usepackage{fontspec}
\newfontfamily\cjkfont{Noto Sans CJK SC}

\usepackage{listings}

\title{Snowbot: An empirical study of building chatbot using seq2seq model with different machine learning framework}

\author{
Pinglei Guo \\
\texttt{piguo@ucsc.edu} \\
\And
Yusi Xiang \\
\texttt{yxiang12@ucsc.edu} \\
\And
Yunzheng Zhang \\
\texttt{yzhan300@ucsc.edu} \\
\And
Weiting Zhan \\
\texttt{wzhan83@ucsc.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
    Chatbot is a growing topic, we built a open domain generative chatbot using seq2seq model with different machine learning framework (Tensorflow, MXNet).
    Our result show although seq2seq is a successful method in neural machine translation, use it solely on single turn chatbot yield pretty unsatisfactory result.
    Also existing free dialog corpus lacks both quality and quantity.
    Our conclusion it's hard to build a useful open domain generative bot using state of art technology.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\subsection{Chatbot}
\label{subsec:chatbot}

Chatbot can be generally divided into two types, open domain and close domain.
Former can answer a wide range of questions (though the answer itself could be very general with syntax and semantic error),
latter can solve particular problems from user, and user is not expecting a close domain bot to chat freely as well.
Based on how response is generated, it can be divided to retrieval and generative,
where answer is picked from clustered responses~\cite{kannan2016smart} or generated on the fly.
In theory, open domain chatbot has more potential\footnote{if the domain is really open enough, generate random text might be the best algorithm},
and is harder to build, in practice, most chatbot that gets work done is close domain.

At first (during proposal) we believe a close domain retrieval chatbot is the most easy one in the total four combinations based on a blog post~\cite{wildml},
Then we realize that this is the most easy one in theory but the hardest one in practice, the other corner,
open domain generative is actually easiest because it's end to end, there is no need for clustering, named entity recognition, knowledge base etc.
And there is no hard metrics for evaluating a open domain chatbot~\cite{liu2016not}.
Thus we decided to build a open domain generative chatbot in the limited time given.

\subsection{Machine Learning Frameworks}
\label{subsec:ml-frameworks}

Machine learning framework is now the competing spot for tech giants (Table~\ref{table:ml-frameworks}), besides they are using ML excessively in their own products,
they want developers to use their framework and run it on their cloud platform, (i.e. Tensorflow on Google Cloud, MXNet on AWS, CNTK on Azure).
Although there were many open source frameworks started by individual and research organizations, most of them are deprecated
in favor of those backed by companies (i.e. Theano vs. Tensorflow)\footnote{\url{https://groups.google.com/forum/\#!msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ}}

\begin{table}[h]
    \caption{Popular Machine learning Frameworks}
    \label{table:ml-frameworks}
    \centering
    \begin{tabular}{llll}
        \toprule
        Name & Star & Company & First Release\\
        \midrule
        \href{https://github.com/tensorflow/tensorflow}{Tensorflow} & 81697 & Google & 2015-11-08 \\
        \href{https://github.com/keras-team/keras}{Keras} & 22872 & N/A & 2015-01-13 \\
        \href{https://github.com/BVLC/caffe}{Caffe} & 21739 & N/A & 2014-03-19 \\
        \href{https://github.com/Microsoft/CNTK}{CNTK} & 13366 & Microsoft & 2016-01-22 \\
        \href{https://github.com/apache/incubator-mxnet}{MXNet} & 12392 & Amazon & 2015-12-09 \\
        \href{https://github.com/pytorch/pytorch}{PyTorch} & 10142 & Facebook & 2016-08-31 \\
        \href{https://github.com/torch/torch7}{Torch} & 7537 & N/A & 2015? \\
        \bottomrule
    \end{tabular}
\end{table}

Tensorflow~\cite{abadi2016tensorflow} and MXNet~\cite{chen2015mxnet} are two widely used frameworks, while tensorflow is
mostly declarative, mxnet allows more imperative programming style.
A simple example is in order to debug the value of a matrix in tensorflow you need to write an op using \verb+tf.Print+ and put it inside compute graph,
while in mxnet you can just use \verb+print+ like it's a normal python variable, though in fact it might resides on GPU or other machines.
Declarative style is not very expressive, but make optimization on the framework side easier, mxnet chose to detect patterns
and only optimize critical path to gain flexibility while retain performance.
However, when it comes to most software engineers, the design and low-level API does not matter much,
community support and a wide range of up to date model is more important, and tensorflow is the obvious winner.
For researchers, mxnet might be a better choice, since it has thinner wrapper and stricter requirement.

We chose to use two higher level seq2seq frameworks, sockeye (based on mxnet)\footnote{\url{https://github.com/awslabs/sockeye}}
and OpenNMT-tf\footnote{\url{https://github.com/OpenNMT/OpenNMT-tf}} in the experiments,
we did try to write a naive one based on newer seq2seq API in tensorflow, however since both sockeye and OpenNMT-tf use state of art models
and yield unsatisfactory result, we abandoned it in the middle \footnote{\url{https://github.com/at15/snowbot/pull/23}}.

The rest of the report is organized as following, Section~\ref{sec:model} describes the basic form of the seq2seq model we are using.
Section~\ref{sec:dataset} shows how we process our dataset, and the problem in it.
Implementation detail is listed in section~\ref{sec:implementation}.
Our result is shown in section~\ref{sec:evaluation}.
Section~\ref{sec:conclusion} concludes the report.

\section{Model}
\label{sec:model}

We use seq2seq model, which is widely used in neural machine translation~\cite{sutskever2014sequence}
and can be applied to single turn dialog system (QA) as well.
Its basic structure is two recurrent neural networks (RNN) as shown in figure~\ref{fig:seq2seq}.
The cell used in RNN is LSTM (long term short memory),
so we can learn implicit relation ship between words from data without explicit pre-processing.
seq2seq is one of many variants of RNN + LSTM. Based on the domain and length of input,
RNN + LSTM can be used for sequence classification, text generation, translation, dialog system etc.
as shown in table~\ref{table:RNN+LSTM}

\begin{table}[h]
    \caption{Variants of RNN + LSTM}
    \label{table:RNN+LSTM}
    \centering
    \begin{tabular}{llll}
        \toprule
        Type & Input & Output & Example \\
        \midrule
        seq2one (classification) & sentence & class & Sentiment analysis \\
        seq2seq (text generation) & sentence & same sentence & Shakespeare style text \\
        seq2seq (translation) & language A & language B & NMT A -> B \\
        seq2seq (single turn QA) & question & answer & chatbot \\
        seq2seq (multi turn QA) & conversation context & answer & chatbot \\
        \bottomrule
    \end{tabular}
\end{table}

For many to many in RNN + LSTM (seq2seq), simply swap the input and output data, you get a different application for free
(Table~\ref{table:seq2seq-app}) \footnote{not real data, for demonstration only}.
So we can use neural machine translation framework directly to build our chatbot.

\begin{table}[h]
    \caption{Input and Output of different seq2seq application}
    \label{table:seq2seq-app}
    \centering
    \begin{tabular}{lllll}
        \toprule
        Type & Train input & Train output & Test input & Test output \\
        \midrule
        text generation & You are my foe & You are my foe & You are & my <unk>\\
        translation & {\cjkfont 你好} & Hello & {\cjkfont 吃了么} & Good morning \\
        single turn QA & Any idea & I don't know & How's the weather & I don't know \\
        multi turn QA & Got it?; No; Why? & I don't know & Hi; Hey; What's up? & I don't know \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{seq2seq-simple}
    \caption{Sequence to Sequence model for single turn Dialog}
    \label{fig:seq2seq}
\end{figure}

From figure~\ref{fig:seq2seq} we can see the main idea of seq2seq is to encode input words sequence into a vector,
and decode this vector until reaches max step or EOS (\verb+</s>+).
On the encoder side, the output and hidden state are all discarded except the last hidden state,
which certainly lose some important information, and leads to optimization like attention~\cite{luong2015effective}.

% TODO: add graph

On the decoder side, train and test (infer) is different.
The first word is \verb+<s>+ instead of real word in both case, but in training, the word feed into each step is from
original data while in testing, the word feed into each step is based on the output of previous step.
The output from projection layer (after softmax) is a vector indicating probability of each word in vocabulary.
In training, we calculate loss use cross entropy, note that due to the length of sequences in a mini batch may not be the same,
we use padding (\verb+<PAD>+) to form a matrix and we need to mask those padded part out when calculating loss.
In test, we can pick the word with max probability as word for this step, this is the greedy way,
but we may not pick the overall best sentence if we consider each word independently.
A better way is to use Beam Search, which is keep top-K words in each step, and take the path of top-1 as the final output.

In sockeye and OpenNMT-tf, both attention and beam search are enabled by default, also there can be more than one layer LSTM
or combination of cells (bi-direction), in original NMT paper they say feed input in reverse order increase performance~\cite{sutskever2014sequence},
and with bi-direction, a sentence is feed into encoder in two directions.

\section{Dataset}
\label{sec:dataset}

\subsection{Handling I don't know in Cornell Movie Dialog Corpus}
\label{subsec:idk}

We start with Cornell Movie Dialog Corpus~\cite{data:cornell} because it is used by many publication and blog posts for single turn dialog system.
It contains 83097 conversations, and can be converted to 221282 single turn QA, total file size is 22MB.

However, our first run using sockeye shows \verb+I don't know+ is the most popular response (Table \ref{table:cornell-idk} left),
which leads to a very simple questions. How many \verb+I don't know+ do we have in training data,
does the bot learn to know what it really knows, or there are just too many \verb+I don't know+ in training data.

So we add counting and removing \verb+I don't know+ when processing text.
There are 5114 response is just \verb+I don't know+, which is 2.3\%., excluding those with \verb+I don't know+ as prefix.
Considering the corpus is from movie with varies backgrounds, 2.3\% is a relative high percentile.
After removing I don't know, the responses (Table~\ref{table:cornell-idk} right) become very general questions like \verb+what do you mean+.

\begin{table}[h]
    \caption{Cornell corpus before and after filtering out I don't know}
    \label{table:cornell-idk}
    \centering
\begin{tabular}{p{70mm}|p{70mm}}
  \toprule
  Raw text without processing & After filtering out I don't know \\
  \hline
  Q: hi \newline
A: I don't want to talk to you. \newline
Q: do you know \newline
A: I don't know. \newline
Q: you know what \newline
A: I don't know. \newline
Q: oh say something other than I don't know \newline
A: What are you talking about? \newline
Q: so this is all you got \newline
A: I don't want to talk about it. \newline
Q: do you like cat \newline
A: I don't know. \newline
Q: what do you know \newline
A: I don't know. \newline
Q: where can I find the park \newline
A: I don't know. \newline
  &
Q: hi \newline
A: hi. \newline
Q: do you know \newline
A: i don't think so. \newline
Q: you know what \newline
A: what do you want? \newline
Q: oh say something other than i don't know \newline
A: what do you want me to do? \newline
Q: so this is all you got \newline
A: i don't know what it is. \newline
Q: do you like cat \newline
A: what do you mean? \newline
Q: what do you know \newline
A: what do you mean? \newline
Q: where can i find the park \newline
A: i don't want to know what you want. \newline
  \\
  \bottomrule
\end{tabular}
\end{table}

\subsection{Twitter Chat log}
\label{subsec:twitter}

Since Cornell Movie Dialog Corpus is quite dramatic, we decided to try more daily style dialog corpus.
The data is from Marsan-Ma/chart\_corpus\footnote{\url{https://github.com/Marsan-Ma/chat\_corpus}}, he wrote
a crawler to scrape twitter.\footnote{As for now, using Twitter API requires enterprise plan and share data on github has legal issues}
It contains 377265 QA and there is just 6 pure \verb+I don't know+, total size is 51MB.

On interesting to note is the data contains emoji, and we didn't remove it when pre-processing.
We could just keep all the ASCII, but we thought it would be interesting to see if emoji would show up in generated response, and they did, as show in Table~\ref{table:twitter}

% TODO: pick the dialog, currently, only thank you will show the the star

\section{Implementation}
\label{sec:implementation}

\subsection{Data Pipeline}
\label{subsec:data-pipeline}

Because we decide to use different frameworks's with different dataset, a lot work is done to provide a unified interface
for processing the data and feed into the graph.
Each framework has their own dataset wrapper and special data format (i.e. \verb+TfRecord+).
Instead of writing ad-hoc code to adapt to different code and dataset,
we generalize this procedure in package \verb+snowbot.corpus+\footnote{\url{https://github.com/at15/snowbot/tree/master/snowbot/corpus}}.
So adding a new dataset just need to inherit the class and implement some dataset specific logic like handling invalid input.
It has a command line interface as shown in figure~\ref{fig:data-pipeline}.

% NOTE: we should have used text instead of image, but it's hard to put
\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{data-pipeline}
    \caption{Data Pipeline command line interface}
    \label{fig:data-pipeline}
\end{figure}

\subsection{Text processing}
\label{subsec:text-processing}

% TODO
TODO: embedding, special token etc.

\subsection{Seq2Seq Frameworks}
\label{subsec:seq2seq-frameworks}

In our experiments we use existing seq2seq frameworks, there are plenty of them, as shown in Table~\ref{table:seq2seq-frameworks}.
We chose OpenNMT-tf and sockeye because we are interested in the rumor that MXNet is more efficient on using GPU memory than Tensorflow.

\begin{table}[h]
    \caption{Seq2Seq Frameworks}
    \label{table:seq2seq-frameworks}
    \centering
    \begin{tabular}{llll}
        \toprule
        Name & Framework & GitHub & Star \\
        \midrule
        OpenNMT-py & PyTorch & \href{https://github.com/OpenNMT/OpenNMT-py}{OpenNMT/OpenNMT-py} & 773 \\
        OpenNMT & Torch & \href{https://github.com/OpenNMT/OpenNMT}{OpenNMT/OpenNMT} & 1461 \\
        OpenNMT-tf & Tensorflow & \href{https://github.com/OpenNMT/OpenNMT-tf}{OpenNMT/OpenNMT-tf} & 145 \\
        nmt & Tensorflow & \href{https://github.com/tensorflow/nmt}{tensorflow/nmt} & 2094 \\
        seq2seq & Tensorflow & \href{https://github.com/google/seq2seq}{google/seq2seq} & 3019 \\
        tensor2tensor & Tensorflow & \href{https://github.com/tensorflow/tensor2tensor}{tensorflow/tensor2tensor} & 2975 \\
        Sockeye & MXNet & \href{https://github.com/awslabs/sockeye}{awslabs/sockeye} & 290 \\
        \bottomrule
    \end{tabular}
\end{table}

Although most of them are built for neural machine translation, we can use them for chatbot directly by swapping the input
and output file as show in Table~\ref{table:seq2seq-app}.

\subsubsection{Sockeye (MXNet)}
\label{subsubsec:sockeye}

Sockeye only requires two text file, it was expecting two different language, but we replace it with questions and answers in same language.
It will generate the vocabulary on the fly, it simply splits sentence into tokens by space, no conversion like remove special token is performed.
(But we did the pre-processing and joined the processed text with space.) It supports attention and beam search.

\subsubsection{OpenNMT-tf (Tensorflow)}
\label{subsubsec:opennmt-tf}

OpenNMT-tf use a YAML file as config, which is similar across all OpenNMT implementation.
It also supports attention and beam search, but its default configuration is not very practical.
It asks you to set train step instead of epoch, and seems don't support when there is no more improvement after k epoch.

\section{Evaluation}
\label{sec:evaluation}

We run our experiment on a normal desktop with a NVIDIA GTX 1080 8GB.
We planned to use AWS, but it took three days to increase quota (initial is 0),
by the time we can run a p2 instance, we already have most result we need.
Normally it took 1h (around 7 epoch) for sockeye to reach the best result, it will continue 3 epoch before stop training.
For OpenNMT-tf, the default config would take 83.7h to finish training (1000000 steps), so we force it to stop after 1h.

\section{Conclusion}
\label{sec:conclusion}

\printbibliography

\section*{Appendix: A}
\label{sec:appendix:a}

\end{document}