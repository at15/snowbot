\documentclass[11pt,letterpaper,twocolumn,oneside]{article}
\usepackage[letterpaper]{geometry}
\geometry{left=1.8cm, top=1.0cm, right=1.8cm, bottom=2.0cm, footskip=.5cm}
% FIXME: the font is really strange
% \usepackage[T1]{fontenc}
% https://tex.stackexchange.com/questions/39227/no-indent-in-the-first-paragraph-in-a-section
\usepackage{indentfirst}
% \setlength\parindent{0pt}

\usepackage{enumitem}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\usepackage{amssymb}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

% http://tex.stackexchange.com/a/230427/114385
\usepackage{hyperref}

\usepackage{graphicx}
\graphicspath{ {images/} }

\usepackage[backend=biber]{biblatex}
\addbibresource{proposal.bib}

\usepackage{sectsty}
\sectionfont{\fontsize{12}{15}\selectfont}

\begin{document}

% from http://latex.org/forum/viewtopic.php?f=5&t=3032
\twocolumn[%
%  Title and authors
  \begin{center}
    {\large An empirical study of machine learning frameworks for building chatbot in cloud} \\
     \vspace{2ex}
     CMPS242 course project proposal Pinglei Guo et al.
  \end{center}
]

\section{Abstract}

Machine learning frameworks is now a new competing spot for tech giants,
especially cloud service providers with GPU or TPU\footnote{https://cloud.google.com/tpu/} support.
Main stream frameworks are all backed by companies,
Google created tensorflow, Amazon chose MXNet, Facebook created PyTorch and Microsoft built CNTK.
Though they are working on open format neural network format ONNX\footnote{http://onnx.ai/},
the cost of implementing and running same model using different framework on different cloud still varies.

We chose to build a chatbot using seq2seq in the experiment,
though it's hard to evaluate performance of a dialogue system\cite{liu2016not}.
It's close related to previous course projects and our main goal is to explore different frameworks
instead of reach good score in a well defined problem.
Also we plan to use it as a base for building a personal knowledge manage assistant in the future.

\section{Background}

\subsection{ChatBot}

Chatbot can be generally divided into two types,
open domain and close domain. The first one can answer a wide range of question
like a real human while the latter is limited to specific area, like booking tickets,
which has more applications in real world.
Also in terms of data used to train bots, and how the bot generate response,
one can rely on existing conversations, and let bot pick answers from existing corpus
based on conversation context, or generate response from scratch.
We chose to build close domain model using a retrieval approach because it's easier.

\subsection{Model}

Since we have used RNN with LSTM in previous text classification assignment,
it's straight forward for use to try it for building a chat bot.
Instead of many to one (sequence to label), now we want sequence to sequence \cite{sutskever2014sequence},
which is quite like translate, except the response is in same language with different meaning
instead of different language with same meaning.

\subsection{Framework}

We have used tensorflow \cite{abadi2016tensorflow} in previous assignments, it is much more convenient
compared to write differentiate by hand and do the back propagate manually.
However, the programming model in tensorflow is more declarative, the python
code simply construct a graph and what tensorflow use is the graph, you can't
add a print statement to print out a shape of a tensor at runtime, unless you
make a extra operation for print and add it to the graph (tf.Print),
which sacrifice programmers' productivity for optimization (graph definition are much
easier to optimize compared with imperative programs).

Frameworks like MXNet \cite{chen2015mxnet} tackled this issue in a mixed manner,
while it allows user to write imperative code, it still construct graph and only
optimize the critical path. It also have a higher level API called Gluon.

Further more, an open exchange format for neural network is proposed \footnote{http://onnx.ai/},
it would be interesting to see what would happens when put a model trained on tesnorflow on MXNet.

\section{Experiment Design}

There are mainly three things we want to observe in the experiment

\begin{itemize}
  \item the difficulty of implementing same well defined model in different frameworks (Tensorflow, MXNet, PyTorch)
  \item the cost of running different frameworks on different cloud, time, money and the efforts for provisioning
  \item distributed training on larger dataset
\end{itemize}

We plan to use three size of corpus for the experiment

\begin{itemize}
  \item Cornell Movie Dialog Corpus 50MB \cite{Danescu-Niculescu-Mizil+Lee:11a}
  \item Ubuntu Dialogue Corpus 1GB \cite{lowe2015ubuntu}
  \item Reddit Comments \footnote{https://www.reddit.com/r/datasets/comments/3bxlg7/i\_have\_every\_publicly\_available\_reddit\_comment}
  or Stackoverflow archive 1TB \footnote{https://archive.org/details/stackexchange}
\end{itemize}

We will start from the smallest dataset, and move to the bigger one, since
we have many frameworks to use, the work can be distributed to team members and
executed in parallel, some preprocessing code is framework agnostic and can be reused.
It would be better if we can abstract common interface, but we would be reinventing
things like Keras \footnote{https://keras.io/}, which is impossible for a 3 weeks project.

As for cloud service providers, we will be using GCP and AWS for larger corpus,
since the data can no longer be fit on local disk, and the training will take a long time.
GCP has TPU, which is not available on other platforms.

\section{Technical Challenges}

Since we are no longer dealing with tiny dataset, performance is becoming a problem.
Especially pre processing text, python, though convenient, is really slow, one way
to solve it is to store pre processed corpus into numeric format and load it directly.
However, this could still be very slow, due to throughput limit of disk.
We might considering use other language and store result in memory to speed up this process if there is time for doing so.

Also for distributed training, we need to figure out the part that can be run in parallel.
There are two types, data parallelism and model parallelism.
Also if we are using different models, (words based LSTM, character based LSTM),
we can simply run it on different machines instead of run them one by one.

\section{Schedule}

\begin{itemize}
  \item 11.26 Have Tensorflow code on Cornell Movie Dialog Corpus running
  \item 11.30 Have MXNet and PyTorch with Cornell Dialog Corpus, Tensorflow with Reddit Comments
  \item 12.05 A Web UI (Flask + Angular) for playing with the bot online
  \item 12.08 Tensorflow and MXNet with larger corpus
  \item 12.12 Wrap up and final report
\end{itemize}

\printbibliography

\end{document}
